import time
import statistics
import tiktoken
import concurrent.futures
from langchain_ollama import ChatOllama
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_core.messages import AIMessage
import pynvml
import threading
import psutil

def count_tokens(text: str) -> int:
    """
    นับจำนวน tokens โดยใช้ tiktoken
    """
    encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))

class SystemMonitor:
    def __init__(self):
        self.monitoring = False
        self._lock = threading.Lock()
        
        # GPU monitoring
        pynvml.nvmlInit()
        self.handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        self.max_memory_used = 0
        self.max_gpu_util = 0
        self.max_memory_percent = 0
        
        # CPU monitoring
        self.max_cpu_util = 0
        self.max_cpu_per_core = [0] * psutil.cpu_count()
        
    def start_monitoring(self):
        """เริ่มการมอนิเตอร์ระบบ"""
        self.monitoring = True
        self.max_memory_used = 0
        self.max_gpu_util = 0
        self.max_memory_percent = 0
        self.max_cpu_util = 0
        self.max_cpu_per_core = [0] * psutil.cpu_count()

    def stop_monitoring(self):
        """หยุดการมอนิเตอร์ระบบ"""
        self.monitoring = False

    def get_system_stats(self):
        """อ่านค่าการใช้งาน GPU และ CPU ปัจจุบัน"""
        try:
            # GPU stats
            memory_info = pynvml.nvmlDeviceGetMemoryInfo(self.handle)
            gpu_util = pynvml.nvmlDeviceGetUtilizationRates(self.handle).gpu
            memory_percent = (memory_info.used / memory_info.total) * 100
            
            # CPU stats
            cpu_util = psutil.cpu_percent()
            cpu_per_core = psutil.cpu_percent(percpu=True)

            with self._lock:
                # อัพเดทค่าสูงสุดของ GPU
                self.max_memory_used = max(self.max_memory_used, memory_info.used)
                self.max_gpu_util = max(self.max_gpu_util, gpu_util)
                self.max_memory_percent = max(self.max_memory_percent, memory_percent)
                
                # อัพเดทค่าสูงสุดของ CPU
                self.max_cpu_util = max(self.max_cpu_util, cpu_util)
                self.max_cpu_per_core = [max(old, new) for old, new in zip(self.max_cpu_per_core, cpu_per_core)]

            return {
                'memory_used': memory_info.used,
                'memory_total': memory_info.total,
                'memory_percent': memory_percent,
                'gpu_util': gpu_util,
                'cpu_util': cpu_util,
                'cpu_per_core': cpu_per_core
            }
            
        except Exception as e:
            print(f"Error getting system stats: {str(e)}")
            return {
                'memory_used': 0,
                'memory_total': 0,
                'memory_percent': 0,
                'gpu_util': 0,
                'cpu_util': 0,
                'cpu_per_core': [0] * psutil.cpu_count()
            }

    def monitor_thread(self):
        """ฟังก์ชันสำหรับ thread ที่คอยมอนิเตอร์ระบบ"""
        while self.monitoring:
            self.get_system_stats()
            time.sleep(0.1)  # ตรวจสอบทุก 100ms

    def get_system_info(self):
        """แสดงข้อมูลทั่วไปของระบบ"""
        try:
            # GPU info
            gpu_name = pynvml.nvmlDeviceGetName(self.handle)
            if isinstance(gpu_name, bytes):
                gpu_name = gpu_name.decode()
            memory_info = pynvml.nvmlDeviceGetMemoryInfo(self.handle)
            total_memory_gb = memory_info.total / (1024**3)
            
            # CPU info
            cpu_count = psutil.cpu_count()
            cpu_freq = psutil.cpu_freq()
            
            return {
                'gpu_name': gpu_name,
                'total_memory_gb': total_memory_gb,
                'cpu_count': cpu_count,
                'cpu_freq_min': cpu_freq.min if cpu_freq else 0,
                'cpu_freq_max': cpu_freq.max if cpu_freq else 0
            }
            
        except Exception as e:
            print(f"Error getting system info: {str(e)}")
            return {
                'gpu_name': 'Unknown',
                'total_memory_gb': 0,
                'cpu_count': psutil.cpu_count(),
                'cpu_freq_min': 0,
                'cpu_freq_max': 0
            }

class TokenSpeedTester:
    def __init__(self, model_name: str, base_url: str):
        self.llm = ChatOllama(
            model=model_name,
            temperature=0.7,
            top_p=0.9,
            top_k=50,
            seed=42,
            base_url=base_url,
            num_ctx=3500,
            num_predict=1200,
            stop=['<|im_end|>'],
            # n_gpu_layers=-1,
            # num_gpu=1,
        )
        
        
        self.prompt = PromptTemplate(
            input_variables=["input_text"],
            template="ตอบคำถาม: {input_text}",
        )
        
        self.llm_chain = LLMChain(llm=self.llm, prompt=self.prompt)
        self.system_monitor = SystemMonitor()
        
        # แสดงข้อมูลระบบเมื่อเริ่มต้น
        sys_info = self.system_monitor.get_system_info()
        print("\nSystem Information:")
        print(f"GPU Model: {sys_info['gpu_name']}")
        print(f"GPU Memory: {sys_info['total_memory_gb']:.2f} GB")
        print(f"CPU Cores: {sys_info['cpu_count']}")
        if sys_info['cpu_freq_max'] > 0:
            print(f"CPU Frequency: {sys_info['cpu_freq_min']:.2f} - {sys_info['cpu_freq_max']:.2f} MHz")
        
    def test_single_query(self, input_text: str) -> dict:
        """
        ทดสอบความเร็วสำหรับ query เดียว พร้อมมอนิเตอร์ระบบ
        """
        input_tokens = count_tokens(input_text)
        
        # เริ่มมอนิเตอร์ระบบ
        self.system_monitor.start_monitoring()
        monitor_thread = threading.Thread(target=self.system_monitor.monitor_thread)
        monitor_thread.start()
        
        start_time = time.time()
        output = self.llm_chain.run(input_text=input_text)
        end_time = time.time()
        
        # หยุดมอนิเตอร์ระบบ
        self.system_monitor.stop_monitoring()
        monitor_thread.join()
        
        output_tokens = count_tokens(output)
        elapsed_time = end_time - start_time
        tokens_per_second = output_tokens / elapsed_time
        
        # แปลงหน่วยความจำเป็น MB
        max_memory_mb = self.system_monitor.max_memory_used / (1024 * 1024)
        
        return {
            "input_text": input_text,
            "output": output,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "elapsed_time": elapsed_time,
            "tokens_per_second": tokens_per_second,
            "max_gpu_memory_mb": max_memory_mb,
            "max_gpu_memory_percent": self.system_monitor.max_memory_percent,
            "max_gpu_util": self.system_monitor.max_gpu_util,
            "max_cpu_util": self.system_monitor.max_cpu_util,
            "max_cpu_per_core": self.system_monitor.max_cpu_per_core
        }
    
    def run_parallel_benchmark(self, test_inputs: list, num_concurrent: int = 4, num_repeats: int = 2) -> dict:
        """
        รัน benchmark test แบบ parallel กับหลาย inputs
        """
        repeated_inputs = test_inputs * num_repeats
        results = []
        detailed_results = []
        
        print(f"\nStarting parallel benchmark with {num_concurrent} concurrent requests...")
        print(f"Total requests to process: {len(repeated_inputs)}")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_concurrent) as executor:
            future_to_input = {executor.submit(self.test_single_query, input_text): input_text 
                             for input_text in repeated_inputs}
            
            for i, future in enumerate(concurrent.futures.as_completed(future_to_input), 1):
                try:
                    result = future.result()
                    results.append(result["tokens_per_second"])
                    detailed_results.append(result)
                    
                    print(f"\nCompleted request {i}/{len(repeated_inputs)}:")
                    print(f"Input: {result['input_text'][:50]}...")
                    print(f"Input tokens: {result['input_tokens']}")
                    print(f"Output tokens: {result['output_tokens']}")
                    print(f"Time taken: {result['elapsed_time']:.2f} seconds")
                    print(f"Tokens per second: {result['tokens_per_second']:.2f}")
                    print(f"GPU Memory Used: {result['max_gpu_memory_mb']:.2f} MB ({result['max_gpu_memory_percent']:.1f}%)")
                    print(f"GPU Utilization: {result['max_gpu_util']:.1f}%")
                    print(f"CPU Utilization: {result['max_cpu_util']:.1f}%")
                    print(f"Max CPU Core Usage: {max(result['max_cpu_per_core']):.1f}%")
                    print(f"Output: {result['output'][:100]}...")
                    
                except Exception as e:
                    print(f"Error processing request: {str(e)}")
        
        # คำนวณสถิติ
        gpu_memory_usage = [r['max_gpu_memory_mb'] for r in detailed_results]
        gpu_memory_percent = [r['max_gpu_memory_percent'] for r in detailed_results]
        gpu_util_rates = [r['max_gpu_util'] for r in detailed_results]
        cpu_util_rates = [r['max_cpu_util'] for r in detailed_results]
        max_cpu_core_rates = [max(r['max_cpu_per_core']) for r in detailed_results]
        
        stats = {
            "average_tps": statistics.mean(results),
            "median_tps": statistics.median(results),
            "std_dev_tps": statistics.stdev(results) if len(results) > 1 else 0,
            "min_tps": min(results),
            "max_tps": max(results),
            "avg_gpu_memory_mb": statistics.mean(gpu_memory_usage),
            "max_gpu_memory_mb": max(gpu_memory_usage),
            "avg_gpu_memory_percent": statistics.mean(gpu_memory_percent),
            "max_gpu_memory_percent": max(gpu_memory_percent),
            "avg_gpu_util": statistics.mean(gpu_util_rates),
            "max_gpu_util": max(gpu_util_rates),
            "avg_cpu_util": statistics.mean(cpu_util_rates),
            "max_cpu_util": max(cpu_util_rates),
            "avg_max_cpu_core": statistics.mean(max_cpu_core_rates),
            "max_cpu_core": max(max_cpu_core_rates),
            "total_queries": len(results),
            "concurrent_requests": num_concurrent,
            "repeats_per_input": num_repeats
        }
        
        return {
            "detailed_results": detailed_results,
            "statistics": stats
        }

# ตัวอย่างการใช้งาน
test_inputs = [
    "อธิบายกระบวนการทำงานของ AI",
    "บอกข้อดีของการเรียนรู้เชิงลึก",
    "ให้ตัวอย่างการใช้งานของ AI ในชีวิตประจำวัน",
    "ทำไมการดูแลรักษาความปลอดภัยข้อมูลถึงสำคัญ",
    "เทคโนโลยีบล็อกเชนทำงานอย่างไร",
]

# สร้าง instance และรัน parallel benchmark
tester = TokenSpeedTester(
    model_name="Pathumma-llm-it-7b-Q4_K_M:latest",
    base_url="http://localhost:7869"
)

# รัน benchmark
results = tester.run_parallel_benchmark(test_inputs, num_concurrent=4, num_repeats=2)

# แสดงผลสรุป
print("\nBenchmark Summary:")
print(f"Performance Metrics:")
print(f"  Average Tokens per Second: {results['statistics']['average_tps']:.2f}")
print(f"  Median Tokens per Second: {results['statistics']['median_tps']:.2f}")
print(f"  Standard Deviation: {results['statistics']['std_dev_tps']:.2f}")
print(f"  Min TPS: {results['statistics']['min_tps']:.2f}")
print(f"  Max TPS: {results['statistics']['max_tps']:.2f}")

print(f"\nGPU Memory Usage:")
print(f"  Average: {results['statistics']['avg_gpu_memory_mb']:.2f} MB ({results['statistics']['avg_gpu_memory_percent']:.1f}%)")
print(f"  Maximum: {results['statistics']['max_gpu_memory_mb']:.2f} MB ({results['statistics']['max_gpu_memory_percent']:.1f}%)")

print(f"\nGPU Utilization:")
print(f"  Average: {results['statistics']['avg_gpu_util']:.1f}%")
print(f"  Maximum: {results['statistics']['max_gpu_util']:.1f}%")

print(f"\nCPU Utilization:")
print(f"  Average Overall: {results['statistics']['avg_cpu_util']:.1f}%")
print(f"  Maximum Overall: {results['statistics']['max_cpu_util']:.1f}%")
print(f"  Average Max Core: {results['statistics']['avg_max_cpu_core']:.1f}%")
print(f"  Highest Core Peak: {results['statistics']['max_cpu_core']:.1f}%")

print(f"\nTest Configuration:")
print(f"  Total Queries: {results['statistics']['total_queries']}")
print(f"  Concurrent Requests: {results['statistics']['concurrent_requests']}")
print(f"  Repeats per Input: {results['statistics']['repeats_per_input']}")



      
